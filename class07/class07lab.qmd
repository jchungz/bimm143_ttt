---
title: "Class 7 : Machine Learning 1"
author: "Jaimy (A16366976)"
format: pdf
---

# Clustering Methods

The broad goal here is to find groupings (clusters) in your input data.

## Kmeans

First, let's make up some data to cluster.

```{r}
x <- rnorm(1000)
hist(x)
```
Make a vector of length 60 with 30 points centerd at -3 and 30 points centered at +3
```{r}
tmp <- c(rnorm(30, mean=-3), rnorm(30, mean=+3))
```
I will now make a wee x and y dataset with 2 groups of points.


```{r}
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

```{r}
k <- kmeans(x, centers=2)
k
```

> Q. From your result object `k` how many points are in each cluster?

```{r}
k$size
```

> Q. What "component" of your result object details the cluster membership?

```{r}
k$cluster
```

> Q. Cluster centers?

```{r}
k$centers
```

> Q. Plot of our clustering results?

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```


```{r}
plot(x, col=18)
```

```{r}
#kmeans
k4 <- kmeans(x, centers = 4)
# plot results
plot(x, col=k4$cluster)
```

A big limitation of kmeans is that it does what you ask even if you ask for silly clusters.

## Hierarchial Clustering

The main base R function for Hierarchial Clustering is `hclust()`. Unlike `kmeans()` you can not just pass it your data as input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Use `plot()` to view results

```{r}
plot(hc)
abline(h=10, col="red")
```

To make the "cut" and get our cluster membership vector we can use the `cutree()` function.

```{r}
grps <- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results

```{r}
plot(x, col=grps)
```

# Principal Component Analysis (PCA)

Here we will do Principal Component Analysis (PCA) on some food data from the UK.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
```

```{r}
#rownames(x) <- x[,1]
#x <- x[, -1]
#x
```

```{r}
dim(x)
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

There are 17 rows and 4 columns.
You can use the dim() function which gives the number of rows and then the number of columns.


## Preview the first 6 rows
```{r}
View(x)
```

```{r}
# Note how the minus indexing works
#rownames(x) <- x[,1]
#x <- x[,-1]
#head(x)
```
```{r}
x <- read.csv(url, row.names=1)
head(x)
```

Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The latter, not the one w -1 because when you run the code w -1 multiple times, it removes a column each time you run it. The latter is much more sufficient and does not remove a column each run.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


Q3: Changing what optional argument in the above barplot() function results in the following plot?

Changing beside to False results in the following plot.


Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

It's a plot of a variable against itself. If a given point lies on the diagonal for a given plot, it means that the x and y values are the same.

```{r}
pairs(x, col=rainbow(10), pch=16)
```

Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

Northern Ireland has more variance, it's more spread out, it's not as linear compared to the other countries

## PCA to the rescue

The main "base" R function for PCA is called `prcomp()`. Here we need to take the transpose of our input as we want the countries in the rows and food as the columns.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

> Q. How much variance is captured in 2 PCs

A. 96.5%

To make our main "PC score plot" or (a.k.a "PC1 vs PC2 plot", or "PC plot" or "ordination plot").
```{r}
attributes(pca)
```

We are after the `pca$x` result component to make out main PCA plot.

```{r}
pca$x
```

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch = 16,
     xlab="PC1 (67.4%)", ylab="PC2 (29%)", xlim=c(-270, 500))
```



Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.


Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

Another important result from PCA is how the original varibales (in this case the foods) contribute to the PCs.

This is contained in the `pca$rotation` object - folks often call this the "loadings" or "contributions" to the PCs.

```{r}
pca$rotation
```

We can make a plot along PC1

```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)

ggplot(contrib) + 
  aes(PC1, rownames(contrib)) +
  geom_col()
```



```{r}
library(ggplot2)

df <- as.data.frame(pca$x)
df_lab <- tibble::rownames_to_column(df, "Country")

# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()
```



```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

The two foods groups that feature prominently are fresh fruit and alcohol drinks. PC2 mainly tells us the 2nd most significant pattern of variability that occurs within the data.



